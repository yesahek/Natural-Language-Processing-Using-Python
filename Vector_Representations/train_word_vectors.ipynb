{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some new raw documents\n",
    "raw_documents = ['I love tacos.',\n",
    "                 'She ran with the chicken.',\n",
    "                 'I dont choose to take a nap. The nap chooses me.',\n",
    "                 'That man is nice as pie with ice cream.',\n",
    "                 'This pizza is an affront to nature.']\n",
    "from nltk.tokenize import word_tokenize\n",
    "def get_tokens(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'love', 'tacos', '.'], ['She', 'ran', 'with', 'the', 'chicken', '.'], ['I', 'dont', 'choose', 'to', 'take', 'a', 'nap', '.', 'The', 'nap', 'chooses', 'me', '.'], ['That', 'man', 'is', 'nice', 'as', 'pie', 'with', 'ice', 'cream', '.'], ['This', 'pizza', 'is', 'an', 'affront', 'to', 'nature', '.']]\n"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "sentences = [get_tokens(r) for r in raw_documents]\n",
    "model = gensim.models.word2vec.Word2Vec(sentences,min_count=1, vector_size= 5)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00593232 -0.15322967  0.1923035   0.09962819  0.18465401]\n",
      "[-0.15019965 -0.01863907  0.19082434 -0.14641772 -0.04663243]\n",
      "[-0.03875482  0.16154873 -0.11861791  0.00090325 -0.09507468]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv['ran'])\n",
    "print(model.wv['love'])\n",
    "print(model.wv['tacos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'filenames', 'target', 'target_names']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "texts = fetch_20newsgroups(subset = 'train')\n",
    "dir (texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Isaac\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\rec.autos\\\\102994'\n",
      " 'C:\\\\Users\\\\Isaac\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.sys.mac.hardware\\\\51861'\n",
      " 'C:\\\\Users\\\\Isaac\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.sys.mac.hardware\\\\51879'\n",
      " ...\n",
      " 'C:\\\\Users\\\\Isaac\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.sys.ibm.pc.hardware\\\\60695'\n",
      " 'C:\\\\Users\\\\Isaac\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.graphics\\\\38319'\n",
      " 'C:\\\\Users\\\\Isaac\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\rec.motorcycles\\\\104440']\n"
     ]
    }
   ],
   "source": [
    "print(texts.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "[7 4 4 ... 3 1 8]\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(len(texts.target))\n",
    "print(texts.target)\n",
    "print(texts.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = texts.data\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', ':', 'lerxst', '@', 'wam.umd.edu', '(', 'where', \"'s\", 'my', 'thing', ')', 'subject', ':', 'what', 'car', 'is', 'this', '!', '?', 'nntp-posting-host', ':', 'rac3.wam.umd.edu', 'organization', ':', 'university', 'of', 'maryland', ',', 'college', 'park', 'lines', ':', '15', 'i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', 'the', 'other', 'day', '.', 'it', 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.', 'it', 'was', 'called', 'a', 'bricklin', '.', 'the', 'doors', 'were', 'really', 'small', '.', 'in', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'this', 'is', 'all', 'i', 'know', '.', 'if', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.', 'thanks', ',', '-', 'il', '--', '--', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst', '--', '--']\n"
     ]
    }
   ],
   "source": [
    "def get_tokens(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return [token.lower() for token in tokens]\n",
    "print (get_tokens(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', ':', 'lerxst', '@', 'wam.umd.edu', '(', 'where', \"'s\", 'my', 'thing', ')', 'subject', ':', 'what', 'car', 'is', 'this', '!', '?', 'nntp-posting-host', ':', 'rac3.wam.umd.edu', 'organization', ':', 'university', 'of', 'maryland', ',', 'college', 'park', 'lines', ':', '15', 'i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', 'the', 'other', 'day', '.', 'it', 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.', 'it', 'was', 'called', 'a', 'bricklin', '.', 'the', 'doors', 'were', 'really', 'small', '.', 'in', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'this', 'is', 'all', 'i', 'know', '.', 'if', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.', 'thanks', ',', '-', 'il', '--', '--', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst', '--', '--']\n"
     ]
    }
   ],
   "source": [
    "# we will treat each document as a sentence\n",
    "sentences = [get_tokens(doc) for doc in data]\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ng = gensim.models.word2vec.Word2Vec(sentences, min_count = 3, vector_size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.7984354496002197),\n",
       " ('god', 0.7297200560569763),\n",
       " ('christ', 0.7209973335266113),\n",
       " ('person', 0.7178294658660889),\n",
       " ('father', 0.7156925797462463),\n",
       " ('himself', 0.70662522315979),\n",
       " ('satan', 0.702955424785614),\n",
       " ('jesus', 0.6984404921531677),\n",
       " ('child', 0.6897723078727722),\n",
       " ('lord', 0.6881251335144043)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ng.wv.most_similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# courps from http//mattmahoney.net/dc/text8.zip\n",
    "sents = gensim.models.word2vec.Text8Corpus('data/text8/text8',max_sentence_length= 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t8 = gensim.models.word2vec.Word2Vec(sents, min_count=5,vector_size = 200, epochs =15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6259205937385559),\n",
       " ('men', 0.5253123044967651),\n",
       " ('girl', 0.5240277647972107),\n",
       " ('creature', 0.5065782070159912),\n",
       " ('person', 0.4900153577327728),\n",
       " ('boy', 0.4898974299430847),\n",
       " ('gentleman', 0.47454026341438293),\n",
       " ('thief', 0.45067816972732544),\n",
       " ('loner', 0.4448513090610504),\n",
       " ('god', 0.43692904710769653)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_t8.wv.most_similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.5894046425819397),\n",
       " ('isabella', 0.5174883008003235),\n",
       " ('matilda', 0.4698965847492218),\n",
       " ('princess', 0.4635002017021179),\n",
       " ('regent', 0.4494529366493225),\n",
       " ('throne', 0.4491971731185913),\n",
       " ('emperor', 0.4478955864906311),\n",
       " ('empress', 0.44239410758018494),\n",
       " ('kings', 0.4414699673652649),\n",
       " ('prince', 0.44097065925598145)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_t8.wv.most_similar(positive=['woman','king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [get_tokens(doc) for doc in texts.data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagged_documents = []\n",
    "for i , sent in enumerate(sentences):\n",
    "    tagged_documents.append(TaggedDocument(sent,['sent {}'.format(i)]))\n",
    "    d2v_model = gensim.models.doc2vec.Doc2Vec(tagged_documents, vector_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.most_similar('fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec0 = d2v_model.infer_vector('i love tacos'.split())\n",
    "print(vec0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.docvecs.most_similar([vec0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
