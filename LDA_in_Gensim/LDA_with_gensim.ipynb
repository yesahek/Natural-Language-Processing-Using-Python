{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA in Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to make clean words and return a list of tokens\n",
    "import spacy\n",
    "parser = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCREEN_NAME', 'said', 'the', '#', 'chicken', 'was', 'at', 'the', '#', 'junkyard', '.', 'see', 'URL', '.']\n"
     ]
    }
   ],
   "source": [
    "sent = '@bob said the #chicken was at the #junkyard. see http://ww.jonathanmugan.com.'\n",
    "out_tokens = tokenize(sent)\n",
    "print(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize so dogs goes to dog and ran goes to run\n",
    "# leammatization means t oget the \"dictionary entry\" for a word\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "# or can use this \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs dog dog\n",
      "ran run ran\n",
      "discouraged discourage discouraged\n"
     ]
    }
   ],
   "source": [
    "for w in ['dogs', 'ran', 'discouraged']:\n",
    "    print(w,get_lemma(w),get_lemma2(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token)> 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token)for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enjoy', 'going', 'restaurant', 'hamburger']\n"
     ]
    }
   ],
   "source": [
    "sent = 'I enjoy going to restaurants to eat hamburgers.'\n",
    "print(prepare_text_for_lda(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buy', 'extend', 'warranty', 'laptop', 'walmart', 'computer', 'break', 'fix', 'hassle']\n",
      "['recently', 'watch', 'robot', 'frank', 'robot', 'older']\n",
      "['become', 'increasingly', 'ambivalent', 'toward', 'malcolm', 'gladwell', 'book', 'abandon', 'david', 'goliath', 'longer', 'trust', 'story']\n",
      "['realization', 'begin', 'thought', 'would', 'month', 'cannotgettherefromhere']\n",
      "['sometimes', 'could', 'wikipedia']\n",
      "['recently', 'watch', 'transcendence', 'appear', 'johnny', 'assume', 'scientist', 'soulless', 'automaton', 'play']\n",
      "['minecraft', 'really', 'store', 'model', 'ecosystem']\n",
      "['single', 'container', 'pollute', 'million', 'truck', 'right']\n",
      "['sneeze', 'still', 'hurt', 'middleage']\n",
      "['interest', 'impression', 'world', 'movie', 'still', 'picture']\n",
      "['printer', 'business', 'asking', 'software', 'update']\n",
      "['spend', 'try', 'little', 'notification', 'icon', 'skinner']\n",
      "['think', 'watch', 'could', 'remote']\n",
      "['excite', 'could', 'would', 'improve', 'improve']\n",
      "['move', 'cheap', 'laptop', 'cloud', 'suddenly', 'resource', 'constrain', 'paybythebit']\n",
      "['presumably', 'drunk', 'repeatedly', 'yelling', 'outside', 'window', 'texas']\n",
      "['beautiful', 'everything', 'come', 'together', 'understanding', 'without', 'befuddlement', 'generate', 'piece']\n",
      "['skull', 'feel', 'especially', 'thick']\n",
      "['concept', 'struggle', 'understand', 'clear', 'wonderful', 'hurt']\n",
      "['playing', 'baseball', 'every', 'position', 'every', 'situation', 'dictate', 'behavior', 'blackjack', 'lookup', 'table']\n",
      "['ignorance', 'programming', 'productivity', 'going', 'never', 'start']\n",
      "['houston', '1800s', 'mosquito', 'cause', 'yellow', 'fever', 'subside', 'yellow', 'fever', 'patient', 'storage']\n",
      "['feeling', 'identify', 'csikszentmihalyi', 'might', 'dangerous', 'innovation', 'often', 'come', 'unenjoyable', 'thought']\n",
      "['dream', 'attack', 'merica', 'drop', 'large', 'style', 'television', 'house', 'helicopter', 'scary', 'sound']\n",
      "['always', 'seem', 'weird', 'baseball', 'crowd', 'perfectly', 'normal']\n",
      "['employee', 'hours', 'accept', 'decline', 'rough', 'sector']\n",
      "['recently', 'watch', 'nightcrawler', 'movie', 'young', 'entrepreneur', 'chase', 'dream']\n",
      "['funny', 'fashion', 'movie', 'wheeler', 'truck', 'movie', 'truck']\n",
      "['family', 'arguing', 'today', 'position', 'always', 'whatever', 'result', 'least', 'crying']\n",
      "['right', 'talking', 'compiler']\n",
      "['seeing', 'crisscross', 'shadow', 'ground', 'branch']\n",
      "['austin', 'traffic', 'usually', 'lighter', 'summer', 'summer']\n",
      "['worship', 'idea', 'hero', 'animal', 'deem', 'favorite']\n",
      "['school', 'learn', 'concept', 'a-->b-->c.', 'boring', 'instead', 'could']\n",
      "['baffled', 'profound', 'concept', 'learn', 'assume', 'eventually', 'figure']\n",
      "['stephen', 'wolfram', 'takeover', 'terminator', 'style', 'follow', 'machine', 'better']\n",
      "['headline', 'meditation', 'devices', 'mindful', 'entrepreneur', 'present', 'presumably', 'seriousness']\n",
      "['price', 'raise', 'planet', 'reduce', 'congestion', 'roads', 'swoop']\n",
      "['emacs', 'years', 'forget', 'binding', 'reduce', 'using', 'remote', 'machine']\n",
      "['dijkstra', 'supposidly', 'picture', 'mind', 'technical', 'start', 'drawing', 'diagram']\n",
      "['someone', 'write', 'browser', 'plugin', 'hide', 'stock', 'photograph', 'clutter', 'information', 'would']\n",
      "['calorie', 'chocolate', 'pancake', 'asking', 'friend', 'totally', 'worth']\n",
      "['finally', 'move', 'couchdb', 'mongodb', 'python', 'documentation', 'difficult', 'query', 'method', 'push']\n",
      "['generally', 'believe', 'purist', 'exist', 'amusement']\n",
      "['luxury', 'silence', 'truck', 'worst', 'follow', 'blower', 'television']\n",
      "['believe', 'still', 'penny', 'worth', 'picking', 'ground']\n",
      "['imagine', 'astley', 'every', 'unexpectedly', 'rickroll', 'everyone']\n",
      "['going', 'seminar', 'leadership', 'seem', 'oxymoron', 'leadership', 'thing', 'byproduct', 'implement', 'change']\n",
      "['tequila', 'make', 'clothes', 'interest', 'sweet', 'would', 'expect', 'title']\n",
      "['complain', 'anything', 'pantry', 'scientist', 'around', 'small', 'notimpressed']\n",
      "['amaze', 'hands', 'interpretive', 'dance']\n",
      "['today', 'stats', 'followers', 'unfollowers', 'followers', 'offend', 'ignore', 'followers', 'annoy', 'followers', 'ask', 'house']\n",
      "['funny', 'little', 'change', 'years', 'school', 'middle', 'night', 'years', 'later', 'still']\n",
      "['would', 'science', 'advance', 'point', 'could', 'predict', 'renting']\n",
      "['protip', 'making', 'flight', 'baltimore', 'airport', 'turkey']\n",
      "['stage', 'someone', 'drive', 'without', 'detail', 'instructions', 'simply', 'close']\n",
      "['arguably', 'arguably', 'claim']\n",
      "['google', 'exactly', 'calorie', 'taco', 'google', 'know']\n",
      "['recently', 'watch', 'movie', 'anyone']\n",
      "['recently', 'years', 'still', 'recognize', 'face', 'human', 'brain', 'amaze']\n",
      "['recur', 'dream', 'movie', 'quite', 'remember']\n",
      "['noticeable', 'forehead', 'maybe', 'drug', 'alien', 'could']\n",
      "['odysseus', 'resist', 'siren', 'song', 'sometimes', 'brush', 'teeth', 'early', 'eating']\n",
      "['today', 'attach', 'blaring', 'little', 'closer', 'idiocracy', 'every']\n",
      "['walking', 'woods', 'behind', 'office', 'stop', 'fence', 'fence', 'spirit', 'truman', 'try']\n",
      "['someday', 'future', 'archaeologist', 'uncover', 'countless', 'credit', 'offer', 'every', 'piece']\n",
      "['dazedly', 'walking', 'around', 'sleepwalking', 'approach', 'punch', 'stomach']\n",
      "['siddhartha', 'could', 'think', 'waiting', 'think', 'hungry', 'longwaytogo']\n",
      "['picture', 'slide', 'convey', 'information', 'presentation', 'appear', 'fluffy', 'context', 'pretty']\n",
      "['network', 'programming', 'somehow', 'satisfy', 'happy', 'little', 'computer', 'talking']\n",
      "['often', 'write', 'incomplete', 'thought', 'forget', 'seeing', 'paper', 'make', 'complete', 'leave']\n",
      "['largely', 'sugar', 'remember', 'giamatti', 'movie', 'soul']\n",
      "['finally', 'watch', 'strangelove', 'great', 'movie', 'scene', 'character', 'strangelove', 'stupid']\n",
      "['enough', 'cramp', 'scream', 'soccer']\n",
      "['using', 'website', 'field', 'false', 'unknowingly', 'friend']\n",
      "['change', 'password', 'often', 'tweet', 'least', 'friend', 'forget']\n",
      "['restaurant', 'taste', 'better', 'compare', 'item', 'compare', 'sandwich']\n",
      "['muppets', 'interest', 'movie', 'small', 'fringe', 'group', 'fighting', 'america', 'dependent', 'foreign']\n",
      "['hunger', 'strike', 'tire']\n",
      "['trail', 'amaze', 'stuff', 'consume', '14,000', 'calorie', 'still', 'hungry']\n",
      "['succinct', 'someone', 'laugh', 'meaning', 'phrase', 'always', 'drift']\n",
      "['worrying', 'exactly', 'right', 'make', 'tax', 'onerous', 'simply', 'filling', 'box', 'judgement', 'would', 'breeze']\n",
      "['dream', 'usually', 'stressful', 'amaze', 'dream', 'traveling', 'distant', 'planet']\n",
      "['somehow', 'life', 'world', 'degree', 'cold']\n",
      "['rule', 'science', 'plant', 'animal', 'liquid', 'glass', 'gravel', 'rock']\n",
      "['computer', 'programming', 'mow', 'enjoy', 'activity', 'either', 'unless']\n",
      "['worth', 'microwave', 'roast', 'mostly', 'plant']\n",
      "['admit', 'wordless', 'wednesday', 'means']\n",
      "['always', 'oscar', 'oscar', 'renta', 'confuse', 'assume', 'sweater']\n",
      "['apparently', 'international', 'every', 'write', 'spanish', 'contain', 'least', 'corazon']\n",
      "['office', 'would', 'raise', 'mailing', 'price', 'could', 'budget', 'problem', 'would', 'fewer', 'credit', 'offer']\n",
      "['sister', 'ask', 'phone', 'night', 'speak', 'mandarin', 'clearly', 'together', 'often']\n",
      "['headache', 'try', 'brain', 'saying', 'enough', 'organize']\n",
      "['reading', 'writing', 'learn', 'anything', 'things', 'mistake']\n",
      "['weird', 'national', 'price', 'submit', 'credentials', 'grocery', 'store']\n",
      "['get', 'older', 'someone', 'something', 'think', 'history', 'place', 'actually']\n",
      "['pennsylvania', 'sunscreen', 'seasonal', 'section', 'texas', 'regular', 'staple', 'butter', 'taco']\n",
      "['whole', 'world', 'wrong', 'means', 'something', 'directly', 'productive', 'prepare', 'later']\n",
      "['understand', 'society', 'blower', 'peace', 'quiet', 'tidiness']\n",
      "['advertising', 'website', 'necessary', 'understand', 'distract', 'movement', 'unusable']\n",
      "['amaze', 'friend', 'dress', 'alike', 'people', 'wearing', 'airport', 'traveling', 'together']\n",
      "['comment', 'bottom', 'article', 'recaptchas', 'code', 'prove', 'human']\n",
      "['would', 'video', 'permanent', 'place', 'things', 'disappear', 'youtube']\n",
      "['want', 'future', 'brother', 'bachelor', 'party', 'married', 'unclearontheconcept']\n",
      "['mathematician', 'drive', 'publish', 'whole', 'something', 'category', 'theory', 'never']\n",
      "['want', 'credit']\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "\n",
    "import random\n",
    "text_data = []\n",
    "with open ('jonathan_mugan_tweets.txt','r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if random.random() > .95:\n",
    "            print(tokens)\n",
    "        text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Isaac\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary from the data\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to bag-of-words corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the corpus and dictionary\n",
    "import pickle\n",
    "pickle.dump(corpus, open(\"corpus.pkl\", \"wb\")) \n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word = dictionary, passes = 15)\n",
    "ldamodel.save('model5.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.010*\"could\" + 0.010*\"going\" + 0.010*\"think\" + 0.008*\"things\"')\n",
      "(1, '0.012*\"reading\" + 0.010*\"coffee\" + 0.008*\"drink\" + 0.008*\"going\"')\n",
      "(2, '0.015*\"people\" + 0.012*\"remember\" + 0.011*\"memory\" + 0.009*\"seem\"')\n",
      "(3, '0.010*\"computer\" + 0.009*\"funny\" + 0.009*\"people\" + 0.007*\"anymore\"')\n",
      "(4, '0.019*\"would\" + 0.010*\"robot\" + 0.009*\"funny\" + 0.007*\"older\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 1), (190, 1)]\n",
      "[(0, 0.72899616), (1, 0.06797663), (2, 0.06674644), (3, 0.06722262), (4, 0.06905815)]\n"
     ]
    }
   ],
   "source": [
    "# try a new document\n",
    "#  topic 3\n",
    "new_doc = 'I watch movies.'\n",
    "new_doc = prepare_text_for_lda(new_doc)\n",
    "new_doc_bow = dictionary.doc2bow(new_doc)\n",
    "print(new_doc_bow)\n",
    "print(ldamodel.get_document_topics(new_doc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.011*\"people\" + 0.008*\"coffee\" + 0.008*\"things\" + 0.007*\"always\"')\n",
      "(1, '0.013*\"could\" + 0.009*\"want\" + 0.006*\"people\" + 0.006*\"would\"')\n",
      "(2, '0.015*\"funny\" + 0.011*\"would\" + 0.010*\"dream\" + 0.009*\"seem\"')\n"
     ]
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3,\n",
    "                                           id2word= dictionary, passes = 15)\n",
    "ldamodel.save('model3.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.017*\"people\" + 0.015*\"email\" + 0.012*\"funny\" + 0.011*\"picture\"')\n",
      "(1, '0.024*\"people\" + 0.017*\"movie\" + 0.015*\"computer\" + 0.013*\"always\"')\n",
      "(2, '0.014*\"people\" + 0.014*\"machine\" + 0.012*\"start\" + 0.012*\"anyone\"')\n",
      "(3, '0.016*\"think\" + 0.016*\"funny\" + 0.015*\"would\" + 0.013*\"change\"')\n",
      "(4, '0.016*\"reading\" + 0.016*\"would\" + 0.013*\"right\" + 0.013*\"going\"')\n",
      "(5, '0.016*\"dream\" + 0.013*\"watch\" + 0.012*\"night\" + 0.011*\"sense\"')\n",
      "(6, '0.014*\"place\" + 0.013*\"soccer\" + 0.012*\"coffee\" + 0.009*\"lunch\"')\n",
      "(7, '0.011*\"problem\" + 0.010*\"stupid\" + 0.009*\"hungry\" + 0.009*\"throw\"')\n",
      "(8, '0.013*\"check\" + 0.010*\"people\" + 0.009*\"somehow\" + 0.009*\"look\"')\n",
      "(9, '0.018*\"remember\" + 0.017*\"would\" + 0.016*\"memory\" + 0.014*\"thing\"')\n"
     ]
    }
   ],
   "source": [
    " # try ten topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10,\n",
    "                                           id2word= dictionary, passes = 15)\n",
    "ldamodel.save('model10.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'filenames', 'target', 'target_names']\n",
      "11314\n",
      "[7 4 4 ... 3 1 8]\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Exercise: Run LDA on Newsgroup Data\n",
    "# The Newsgroup Data\n",
    "# http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#newsgroups\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "texts = fetch_20newsgroups(subset='train')\n",
    "print(dir(texts))\n",
    "# 11,314 posts\n",
    "print(len(texts.target))\n",
    "print(texts.target)\n",
    "print(texts.target_names)\n",
    "print(texts.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
